### 3.4 工程环境：Conda/Docker、日志与可视化

#### 环境管理：让代码在任何地方都能跑

**Conda**

Conda的基本操作：
```bash
# 创建新环境（相当于给项目一个独立房间）
conda create -n robotsim python=3.9
# -n 是name的缩写，robotsim是环境名字，随便起

# 激活环境（进入这个房间）
conda activate robotsim
# 你会看到命令行前面多了 (robotsim)

# 查看所有环境
conda env list
# 带*号的是当前激活的

# 删除不要的环境
conda remove -n old_env --all
```

安装PyTorch的正确姿势：
```bash
# 先看你的CUDA版本
nvidia-smi  # 右上角会显示CUDA Version: 11.8

# 去PyTorch官网(pytorch.org)选对应版本
# 比如CUDA 11.8：
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# 验证安装
python -c "import torch; print(torch.cuda.is_available())"  # 应该输出True
```

环境导出和复现（重要！）：
```bash
# 导出当前环境到yml文件
conda env export > environment.yml

# 但这个文件太详细了，跨平台可能有问题
# 更好的方式是只导出你explicitly安装的包：
conda env export --from-history > environment.yml

# 别人拿到你的yml文件后：
conda env create -f environment.yml
```

Conda和pip混用的技巧：
```python
# 先用conda装大件（numpy, scipy, pytorch这些）
conda install numpy scipy matplotlib

# 再用pip装小包或conda没有的
pip install transformers datasets

# 为啥这个顺序？
# conda会处理系统级依赖（比如MKL、CUDA），pip不会
# 如果先pip后conda，可能会破坏pip装的包
```

**Docker：终极解决方案**

Conda还是可能出问题（比如系统库不同），Docker是真正的"带环境运行"。

最简单的Dockerfile：
```dockerfile
# 基础镜像（别人已经装好PyTorch的）
FROM pytorch/pytorch:2.0.1-cuda11.8-cudnn8-runtime

# 设置工作目录
WORKDIR /workspace

# 复制代码
COPY . .

# 安装额外依赖
RUN pip install transformers wandb

# 运行命令
CMD ["python", "train.py"]
```

构建和运行：
```bash
# 构建镜像
docker build -t my_robot_model .
# -t 是tag，给镜像起个名字

# 运行容器
docker run --gpus all -it my_robot_model
# --gpus all 让容器能用GPU
# -it 是交互模式，能看到输出

# 挂载本地目录（开发时常用）
docker run --gpus all -v /home/user/data:/workspace/data -it my_robot_model
# -v 是volume，把本地/home/user/data挂载到容器的/workspace/data
```

Docker的坑：
1. Windows上路径要用绝对路径，斜杠方向要注意
2. GPU支持需要装nvidia-docker
3. 镜像会很大（几个G），注意硬盘空间

#### 实验追踪：WandB让你知道哪次实验效果好

训练模型最怕的：跑了100次实验，不记得哪次参数效果最好...

**WandB（Weights & Biases）入门**

```python
import wandb

# 初始化（第一次要去wandb.ai注册账号）
wandb.init(
    project="robot-grasping",  # 项目名
    name="exp-2024-01-15",      # 本次实验名字
    config={
        "learning_rate": 1e-3,
        "batch_size": 32,
        "model": "resnet50",
        "epochs": 100
    }
)

# 训练循环中记录
for epoch in range(100):
    train_loss = train_one_epoch()
    val_loss, val_acc = validate()

    # 记录到wandb
    wandb.log({
        "train_loss": train_loss,
        "val_loss": val_loss,
        "val_acc": val_acc,
        "epoch": epoch
    })

    # 还能记录图片
    if epoch % 10 == 0:
        fig = plot_predictions()
        wandb.log({"predictions": wandb.Image(fig)})

# 结束
wandb.finish()
```

WandB会自动生成漂亮的可视化界面，能看到：
- 损失曲线
- 学习率变化
- 系统资源（GPU利用率、显存）
- 你记录的所有图片

更高级的用法：
```python
# 超参数搜索
sweep_config = {
    'method': 'bayes',  # 贝叶斯优化
    'parameters': {
        'learning_rate': {'min': 1e-5, 'max': 1e-2},
        'batch_size': {'values': [16, 32, 64]},
        'dropout': {'min': 0.1, 'max': 0.5}
    }
}

sweep_id = wandb.sweep(sweep_config, project="robot-grasping")
wandb.agent(sweep_id, train_function, count=50)  # 跑50次实验
```

**TensorBoard（备选）**

如果不想用云服务，TensorBoard是本地方案：
```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/exp1')

for epoch in range(100):
    loss = train()
    writer.add_scalar('Loss/train', loss, epoch)

    # 记录模型权重分布
    for name, param in model.named_parameters():
        writer.add_histogram(name, param, epoch)

writer.close()

# 查看：tensorboard --logdir=runs
```

#### 实验可重复性：让结果能复现

论文里说准确率95%，你跑出来85%，是不是很郁闷？

**固定随机种子**
```python
import random
import numpy as np
import torch

def set_seed(seed=42):
    """完全固定随机性"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # 多GPU

    # 这两个会降低性能，但保证完全可重复
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# 在代码最开始调用
set_seed(2024)
```

但注意，完全固定会让训练变慢（特别是cudnn.benchmark=False）。折中方案：
```python
def set_seed_loose(seed=42):
    """部分固定，性能更好"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

    # 不设置这两个，速度快但可能有细微差异
    # torch.backends.cudnn.deterministic = True
    # torch.backends.cudnn.benchmark = False
```

**配置文件管理**

别把超参数硬编码在代码里：
```yaml
# config.yaml
model:
  name: resnet50
  pretrained: true
  num_classes: 10

training:
  batch_size: 32
  learning_rate: 0.001
  epochs: 100
  optimizer: adam

data:
  train_path: /data/train
  val_path: /data/val
  num_workers: 4
```

读取配置：
```python
import yaml
from argparse import ArgumentParser

# 读取yaml
with open('config.yaml') as f:
    config = yaml.load(f, Loader=yaml.FullLoader)

# 命令行参数覆盖配置文件
parser = ArgumentParser()
parser.add_argument('--batch_size', type=int, default=config['training']['batch_size'])
parser.add_argument('--lr', type=float, default=config['training']['learning_rate'])
args = parser.parse_args()

# 使用
batch_size = args.batch_size
learning_rate = args.lr
```

#### 代码组织：专业的项目结构

别把所有代码都塞在一个train.py里！

标准项目结构：
```
robot_grasping/
├── configs/
│   ├── default.yaml      # 默认配置
│   └── experiments/       # 不同实验配置
│       ├── resnet50.yaml
│       └── vit.yaml
├── data/
│   ├── __init__.py
│   ├── dataset.py        # 数据集定义
│   └── transforms.py     # 数据增强
├── models/
│   ├── __init__.py
│   ├── backbone.py       # 特征提取器
│   ├── head.py          # 任务头
│   └── losses.py        # 损失函数
├── utils/
│   ├── __init__.py
│   ├── metrics.py       # 评价指标
│   ├── visualization.py # 可视化
│   └── logger.py        # 日志
├── scripts/
│   ├── train.py         # 训练脚本
│   ├── evaluate.py      # 评估脚本
│   └── inference.py     # 推理脚本
├── notebooks/           # Jupyter实验
│   └── explore_data.ipynb
├── tests/              # 单元测试
│   └── test_model.py
├── README.md
├── requirements.txt
└── setup.py            # 包安装配置
```

让代码可安装：
```python
# setup.py
from setuptools import setup, find_packages

setup(
    name="robot_grasping",
    version="0.1.0",
    packages=find_packages(),
    install_requires=[
        "torch>=2.0.0",
        "numpy>=1.20.0",
        "wandb",
    ]
)

# 这样就能 pip install -e . 安装你的包
# 然后在任何地方 import robot_grasping
```

#### 调试技巧：快速定位问题

**打印shape是第一步**
```python
def forward(self, x):
    print(f"Input shape: {x.shape}")
    x = self.conv1(x)
    print(f"After conv1: {x.shape}")
    x = self.pool(x)
    print(f"After pool: {x.shape}")
    # 很土但很有效！
```

**用assert检查假设**
```python
def process_batch(images, labels):
    assert images.dim() == 4, f"Expected 4D tensor, got {images.dim()}D"
    assert images.shape[0] == labels.shape[0], "Batch size mismatch"
    assert images.min() >= 0 and images.max() <= 1, "Images not normalized"
    # ...
```

**梯度检查**
```python
# 检查梯度是否正常
for name, param in model.named_parameters():
    if param.grad is not None:
        grad_norm = param.grad.norm().item()
        if grad_norm > 100:
            print(f"Warning: large gradient in {name}: {grad_norm}")
        if grad_norm == 0:
            print(f"Warning: zero gradient in {name}")
```

**常见错误和解决**

1. **CUDA out of memory**
```python
# 方法1：减小batch size
batch_size = 16  # 从32改到16

# 方法2：梯度累积
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# 方法3：删除不用的变量
del intermediate_output
torch.cuda.empty_cache()
```

2. **Loss变成NaN**
```python
# 检查输入
assert not torch.isnan(inputs).any()

# 梯度裁剪
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 检查学习率
if epoch > 50:
    for param_group in optimizer.param_groups:
        param_group['lr'] *= 0.1
```

3. **DataLoader卡死**
```python
# Windows上容易出这个问题
DataLoader(dataset, num_workers=0)  # 改成0

# Linux上可以用更多workers
DataLoader(dataset, num_workers=4, pin_memory=True)
```
