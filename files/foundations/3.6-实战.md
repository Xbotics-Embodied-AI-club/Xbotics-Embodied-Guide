### 3.6 实战与应用

#### 多模态融合：让机器人有多种感知

机器人不只有眼睛（相机），还有触觉、关节编码器、力传感器...怎么把这些信息融合起来？

**统一到同一个特征空间**

最简单的思路——把所有模态都编码成相同维度的向量：

```python
class MultiModalEncoder(nn.Module):
    def __init__(self, d_model=512):
        super().__init__()
        # 不同模态用不同编码器
        self.vision_encoder = nn.Linear(2048, d_model)  # ResNet特征
        self.text_encoder = nn.Embedding(10000, d_model)  # 词嵌入
        self.joint_encoder = nn.Linear(7, d_model)  # 7自由度机械臂
        self.force_encoder = nn.Linear(6, d_model)  # 6轴力传感器

    def forward(self, vision=None, text=None, joints=None, force=None):
        tokens = []

        if vision is not None:
            # vision: [batch, 2048]
            vision_tokens = self.vision_encoder(vision)  # [batch, d_model]
            tokens.append(vision_tokens.unsqueeze(1))  # [batch, 1, d_model]

        if text is not None:
            # text: [batch, seq_len]
            text_tokens = self.text_encoder(text)  # [batch, seq_len, d_model]
            tokens.append(text_tokens)

        if joints is not None:
            # joints: [batch, 7]
            joint_tokens = self.joint_encoder(joints)
            tokens.append(joint_tokens.unsqueeze(1))

        if force is not None:
            # force: [batch, 6]
            force_tokens = self.force_encoder(force)
            tokens.append(force_tokens.unsqueeze(1))

        # 拼接所有模态
        multi_modal = torch.cat(tokens, dim=1)  # [batch, total_seq_len, d_model]
        return multi_modal
```

然后扔给Transformer处理，它会自动学习跨模态关系！

**跨模态注意力的实际效果**

训练后你会发现神奇的现象：
- 当文本说"抓取红色方块"，视觉注意力会聚焦在红色区域
- 当力传感器检测到碰撞，文本理解会关注"小心"、"慢速"这些词
- 关节接近限位时，规划模块会降低相应方向的运动权重

**处理不同采样率**

现实中各传感器频率不同：
- 相机：30Hz
- 关节编码器：100Hz
- 力传感器：1000Hz

怎么对齐？

```python
# 方法1：下采样到最低频率（简单但损失信息）
def align_to_vision_rate(vision_seq, joint_seq, force_seq):
    # vision_seq: [T_v=30, ...]
    # joint_seq: [T_j=100, ...]
    # force_seq: [T_f=1000, ...]

    # 下采样到30Hz
    joint_downsampled = joint_seq[::3]    # 每3个取1个，100Hz→33Hz≈30Hz
    force_downsampled = force_seq[::33]   # 每33个取1个，1000Hz→30Hz

    return vision_seq, joint_downsampled, force_downsampled

# 方法2：分层处理（保留高频信息）
class HierarchicalFusion(nn.Module):
    def __init__(self):
        super().__init__()
        # 高频融合器（1000Hz）
        self.force_processor = nn.LSTM(6, 64)
        # 中频融合器（100Hz）
        self.joint_force_fusion = nn.LSTM(64+7, 128)
        # 低频融合器（30Hz）
        self.full_fusion = nn.LSTM(128+2048, 512)

    def forward(self, vision, joints, force):
        # Step1: 处理高频力信号（1000Hz）
        force_features, _ = self.force_processor(force)  # [1000, 64]
        # 池化到100Hz
        force_100hz = F.adaptive_avg_pool1d(
            force_features.transpose(0, 1).unsqueeze(0),
            output_size=100
        ).squeeze(0).transpose(0, 1)

        # Step2: 融合中频信号（100Hz）
        joint_force = torch.cat([joints, force_100hz], dim=-1)
        mid_features, _ = self.joint_force_fusion(joint_force)
        # 池化到30Hz
        mid_30hz = mid_features[::3]

        # Step3: 融合所有模态（30Hz）
        full_input = torch.cat([vision, mid_30hz], dim=-1)
        output, _ = self.full_fusion(full_input)

        return output
```

**实际场景的多模态策略**

插USB（需要力反馈）：
```python
def usb_insertion_policy(vision, force, joint):
    # 阶段1：视觉引导接近
    if not in_contact(force):
        action = vision_servo(vision, target="usb_port")

    # 阶段2：接触后切换到力控
    else:
        # 维持小的插入力，调整姿态
        action = force_control(
            force,
            target_force=[0, 0, 5, 0, 0, 0],  # Z方向5N
            compliance=[0.1, 0.1, 0.01, 0.5, 0.5, 0.5]  # XY柔顺，Z硬
        )

    return action
```

#### 图神经网络（GAT）：理解场景关系

机器人要理解的不只是"有什么"，还要理解"谁和谁有什么关系"。

**场景表示成图**

想象桌面场景：
- 节点：每个物体（杯子、书、笔）
- 边：物体间的关系（接触、遮挡、支撑）

```python
class SceneGraph:
    def __init__(self):
        self.objects = []  # 节点列表
        self.relations = []  # 边列表

    def add_object(self, obj_id, features):
        """添加物体节点"""
        self.objects.append({
            'id': obj_id,
            'features': features,  # 位置、颜色、形状等
            'type': detect_object_type(features)
        })

    def add_relation(self, obj1_id, obj2_id, rel_type):
        """添加关系边"""
        self.relations.append({
            'source': obj1_id,
            'target': obj2_id,
            'type': rel_type  # 'on_top_of', 'occluding', 'near'等
        })

# 实际使用
scene = SceneGraph()
scene.add_object('cup_1', cup_features)
scene.add_object('table_1', table_features)
scene.add_relation('cup_1', 'table_1', 'on_top_of')
```

**图注意力机制（GAT）**

GAT让每个节点通过注意力机制聚合邻居信息：

```python
class GraphAttentionLayer(nn.Module):
    def __init__(self, in_dim, out_dim, dropout=0.1):
        super().__init__()
        self.W = nn.Linear(in_dim, out_dim, bias=False)
        self.a = nn.Parameter(torch.randn(2 * out_dim, 1))
        self.dropout = nn.Dropout(dropout)
        self.leakyrelu = nn.LeakyReLU(0.2)

    def forward(self, node_features, adjacency):
        # node_features: [N, in_dim]
        # adjacency: [N, N] 邻接矩阵

        N = node_features.shape[0]
        h = self.W(node_features)  # [N, out_dim]

        # 计算注意力分数
        # 这里用的是Veličković et al. 2018的方法
        h_i = h.unsqueeze(1).repeat(1, N, 1)  # [N, N, out_dim]
        h_j = h.unsqueeze(0).repeat(N, 1, 1)  # [N, N, out_dim]
        concat = torch.cat([h_i, h_j], dim=-1)  # [N, N, 2*out_dim]

        e = self.leakyrelu(torch.matmul(concat, self.a).squeeze(-1))  # [N, N]

        # 只保留存在边的注意力
        zero_vec = -9e15 * torch.ones_like(e)
        attention = torch.where(adjacency > 0, e, zero_vec)
        attention = F.softmax(attention, dim=-1)
        attention = self.dropout(attention)

        # 聚合邻居特征
        h_prime = torch.matmul(attention, h)  # [N, out_dim]
        return F.elu(h_prime)
```

**实际应用：抓取规划**

用GAT理解场景，找出最容易抓取的物体：

```python
class GraspPlanner(nn.Module):
    def __init__(self):
        super().__init__()
        self.gat1 = GraphAttentionLayer(512, 256)
        self.gat2 = GraphAttentionLayer(256, 128)
        self.grasp_score = nn.Linear(128, 1)  # 输出抓取分数

    def forward(self, scene_graph):
        # 提取特征和邻接矩阵
        node_feats = torch.stack([obj['features'] for obj in scene_graph.objects])
        adj_matrix = build_adjacency_matrix(scene_graph.relations)

        # 两层GAT理解场景关系
        x = self.gat1(node_feats, adj_matrix)
        x = self.gat2(x, adj_matrix)

        # 给每个物体打分
        scores = self.grasp_score(x)  # [N, 1]

        return scores

# 使用时
planner = GraspPlanner()
scores = planner(scene_graph)
best_object_idx = torch.argmax(scores)
# 得分考虑了：
# - 物体本身是否易抓（形状、大小）
# - 是否被其他物体遮挡
# - 抓取后是否会导致其他物体掉落
```

**GAT在机器人中的典型应用**

1. **装配任务**：理解零件之间的装配顺序和约束
2. **导航规划**：理解房间连接关系，找最短路径
3. **人机协作**：理解人的动作意图和物体关系
4. **故障诊断**：通过组件关系图定位问题源头

关键优势：
- 自动学习"什么关系重要"（注意力权重）
- 处理不规则结构（不像CNN需要网格）
- 可解释性好（能可视化注意力）